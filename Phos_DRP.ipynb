{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports: Done\n",
      "Data Imports and Wrangling: Done\n",
      "cutoff percentile:  95\n",
      "functional score cutoff:  0.577659576369025\n",
      "Fraction of cls in sets, relative to all cls before missing values are removed\n",
      "train fraction 0.7317073170731707, test fraction 0.2682926829268293\n",
      "------\n",
      "Fraction of cls in sets, relative to all cl drug pairs, after missing values are removed\n",
      "train fraction 0.6352432571944767, test fraction 0.23286875725900116\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "# import python libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from scripts.splitting import split\n",
    "from scripts.data_wrangling import import_phos\n",
    "from scripts.feature_selection import fs_functional_score, fs_atlas_landmark, true_phosphosite_filter\n",
    "from scripts.data_preparation import data_prep\n",
    "from scripts.data_selection import data_indexing\n",
    "from scripts.mean_model import MeanModel\n",
    "from scripts.tf_DRP_model import train_model, train_model_cv\n",
    "\n",
    "tf.keras.utils.set_random_seed(42) # set random seeds for Python, NumPy, and TensorFlow\n",
    "\n",
    "print('Imports: Done')\n",
    "\n",
    "\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "# Import and wrangle phosphoproteomics and GDSC1 drug data\n",
    "phos_df, drug_df, drug_matrix, _all_cls, _all_drugs, common_ind  = import_phos()\n",
    "\n",
    "## Feature Selection \n",
    "\n",
    "# create list of landmark phosphosites\n",
    "phosphosites = fs_functional_score(phos_df,95)\n",
    "#phosphosites = fs_atlas_landmark(phos_df,80)\n",
    "\n",
    "# remove false positive phosphosites\n",
    "filtered_phosphosites = true_phosphosite_filter(phosphosites)\n",
    "\n",
    "# reduce remaining features down by feature selection\n",
    "phos_df = phos_df.filter(filtered_phosphosites,axis=1)\n",
    "\n",
    "## Data Preparation & Selection\n",
    "x_drug, x_all, y_series = data_prep(drug_df,phos_df,common_ind)\n",
    "drug_cl_pairs =  y_series.index\n",
    "\n",
    "# Cancer blind train-test split\n",
    "rand_seed = 42 # set random seed for train-test split\n",
    "train_size = 0.75 # set train-test fraction\n",
    "\n",
    "train_pairs, test_pairs = split(rand_seed, _all_cls, _all_drugs, drug_cl_pairs, \n",
    "                                train_size, split_type='cblind')\n",
    "\n",
    "# index for each set of train-test pairs\n",
    "xo_train, xd_train, y_train, xo_test, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all,x_drug,y_series) # select train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Building: Done\n"
     ]
    }
   ],
   "source": [
    "## Model Building\n",
    "\n",
    "# Learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "## CNN using functional API\n",
    "def build_model(learning_rate=1e-2, momentum=0.9, seed=42):\n",
    "\n",
    "    # set weight initialiser\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    \n",
    "    # phosphoproteomics data input\n",
    "    x_input = layers.Input(shape=(xo_train.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1,seed=seed)(x)\n",
    "    # 2nd convolution layer\n",
    "    x = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1,seed=seed)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5,seed=seed)(x)\n",
    "    x = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5,seed=seed)(x)\n",
    "    \n",
    "    # one-hot encoded drug data + dense layer\n",
    "    y_input = layers.Input(shape=(xd_train.shape[1]))\n",
    "    y = Dense(64, kernel_initializer=initializer, activation=\"relu\")(y_input) \n",
    "    \n",
    "    # Concatenate phosphoproteomics and encoded drug data\n",
    "    z = layers.concatenate([x, y])\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(1, kernel_initializer=initializer)(z)\n",
    "\n",
    "    model = tf.keras.Model([x_input, y_input], z)\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print('Model Building: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "326/326 [==============================] - 19s 27ms/step - loss: 0.6894 - mae: 0.6111 - val_loss: 308.2338 - val_mae: 15.2312 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.4172 - mae: 0.4954 - val_loss: 814.5728 - val_mae: 2.1061 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.3776 - mae: 0.4703 - val_loss: 12307.8477 - val_mae: 11.8866 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3513 - mae: 0.4520 - val_loss: 45728.7812 - val_mae: 58.7143 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3709 - mae: 0.4649 - val_loss: 1824.0634 - val_mae: 9.1770 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3697 - mae: 0.4673 - val_loss: 1224.3070 - val_mae: 5.5992 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.3533 - mae: 0.4570 - val_loss: 10947.1504 - val_mae: 31.6953 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3614 - mae: 0.4592 - val_loss: 38728.5273 - val_mae: 14.5096 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3602 - mae: 0.4634 - val_loss: 40593.9102 - val_mae: 56.5110 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3706 - mae: 0.4717 - val_loss: 6463.3965 - val_mae: 12.6521 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3401 - mae: 0.4485 - val_loss: 253.4259 - val_mae: 1.1774 - lr: 0.0090\n",
      "Epoch 12/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3307 - mae: 0.4410 - val_loss: 21504148.0000 - val_mae: 926.5225 - lr: 0.0082\n",
      "Epoch 13/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.3361 - mae: 0.4429 - val_loss: 15762.2334 - val_mae: 9.8259 - lr: 0.0074\n",
      "Epoch 14/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3025 - mae: 0.4207 - val_loss: 44271.6094 - val_mae: 38.6160 - lr: 0.0067\n",
      "Epoch 15/100\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 0.3198 - mae: 0.4337 - val_loss: 0.2898 - val_mae: 0.4150 - lr: 0.0061\n",
      "Epoch 16/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2985 - mae: 0.4173 - val_loss: 0.2815 - val_mae: 0.4051 - lr: 0.0055\n",
      "Epoch 17/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2889 - mae: 0.4106 - val_loss: 15.6755 - val_mae: 0.5116 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2812 - mae: 0.4041 - val_loss: 32.3883 - val_mae: 0.6945 - lr: 0.0045\n",
      "Epoch 19/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2864 - mae: 0.4088 - val_loss: 5.6635 - val_mae: 0.4808 - lr: 0.0041\n",
      "Epoch 20/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2619 - mae: 0.3875 - val_loss: 1.5323 - val_mae: 0.4209 - lr: 0.0037\n",
      "Epoch 21/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2577 - mae: 0.3841 - val_loss: 4.7922 - val_mae: 0.5109 - lr: 0.0033\n",
      "Epoch 22/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2539 - mae: 0.3824 - val_loss: 1.1877 - val_mae: 0.4262 - lr: 0.0030\n",
      "Epoch 23/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2459 - mae: 0.3733 - val_loss: 0.5256 - val_mae: 0.4683 - lr: 0.0027\n",
      "Epoch 24/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2355 - mae: 0.3662 - val_loss: 0.2553 - val_mae: 0.3747 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2401 - mae: 0.3680 - val_loss: 0.2827 - val_mae: 0.3938 - lr: 0.0022\n",
      "Epoch 26/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2380 - mae: 0.3685 - val_loss: 0.5055 - val_mae: 0.3817 - lr: 0.0020\n",
      "Epoch 27/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2409 - mae: 0.3699 - val_loss: 0.2546 - val_mae: 0.3710 - lr: 0.0018\n",
      "Epoch 28/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2371 - mae: 0.3667 - val_loss: 0.2754 - val_mae: 0.3915 - lr: 0.0017\n",
      "Epoch 29/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2346 - mae: 0.3649 - val_loss: 0.2508 - val_mae: 0.3712 - lr: 0.0015\n",
      "Epoch 30/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2252 - mae: 0.3574 - val_loss: 0.2495 - val_mae: 0.3703 - lr: 0.0014\n",
      "Epoch 31/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2298 - mae: 0.3593 - val_loss: 0.2529 - val_mae: 0.3739 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2326 - mae: 0.3627 - val_loss: 0.2590 - val_mae: 0.3768 - lr: 0.0011\n",
      "Epoch 33/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2226 - mae: 0.3554 - val_loss: 0.2568 - val_mae: 0.3770 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2236 - mae: 0.3552 - val_loss: 0.2593 - val_mae: 0.3804 - lr: 9.0718e-04\n",
      "Epoch 35/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2260 - mae: 0.3576 - val_loss: 0.2510 - val_mae: 0.3746 - lr: 8.2085e-04\n",
      "Epoch 36/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2183 - mae: 0.3503 - val_loss: 0.2659 - val_mae: 0.3730 - lr: 7.4273e-04\n",
      "Epoch 37/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2207 - mae: 0.3533 - val_loss: 0.2536 - val_mae: 0.3768 - lr: 6.7205e-04\n",
      "Epoch 38/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2205 - mae: 0.3531 - val_loss: 0.2502 - val_mae: 0.3717 - lr: 6.0810e-04\n",
      "Epoch 39/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2184 - mae: 0.3517 - val_loss: 0.2625 - val_mae: 0.3856 - lr: 5.5023e-04\n",
      "Epoch 40/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2162 - mae: 0.3500 - val_loss: 0.2548 - val_mae: 0.3740 - lr: 4.9787e-04\n",
      "Epoch 41/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2226 - mae: 0.3536 - val_loss: 0.2504 - val_mae: 0.3715 - lr: 4.5049e-04\n",
      "Epoch 42/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2217 - mae: 0.3522 - val_loss: 0.2485 - val_mae: 0.3700 - lr: 4.0762e-04\n",
      "Epoch 43/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2173 - mae: 0.3508 - val_loss: 0.2551 - val_mae: 0.3731 - lr: 3.6883e-04\n",
      "Epoch 44/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2175 - mae: 0.3483 - val_loss: 0.2504 - val_mae: 0.3701 - lr: 3.3373e-04\n",
      "Epoch 45/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2156 - mae: 0.3473 - val_loss: 0.2512 - val_mae: 0.3735 - lr: 3.0197e-04\n",
      "Epoch 46/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2142 - mae: 0.3476 - val_loss: 0.2504 - val_mae: 0.3730 - lr: 2.7324e-04\n",
      "Epoch 47/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2178 - mae: 0.3498 - val_loss: 0.2490 - val_mae: 0.3705 - lr: 2.4723e-04\n",
      "Epoch 48/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2165 - mae: 0.3492 - val_loss: 0.2497 - val_mae: 0.3709 - lr: 2.2371e-04\n",
      "Epoch 49/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2114 - mae: 0.3453 - val_loss: 0.2503 - val_mae: 0.3713 - lr: 2.0242e-04\n",
      "Epoch 50/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2184 - mae: 0.3524 - val_loss: 0.2508 - val_mae: 0.3722 - lr: 1.8316e-04\n",
      "Epoch 51/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2187 - mae: 0.3503 - val_loss: 0.2498 - val_mae: 0.3713 - lr: 1.6573e-04\n",
      "Epoch 52/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2174 - mae: 0.3502 - val_loss: 0.2476 - val_mae: 0.3690 - lr: 1.4996e-04\n",
      "Epoch 53/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2148 - mae: 0.3478 - val_loss: 0.2491 - val_mae: 0.3699 - lr: 1.3569e-04\n",
      "Epoch 54/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2145 - mae: 0.3483 - val_loss: 0.2522 - val_mae: 0.3739 - lr: 1.2277e-04\n",
      "Epoch 55/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2124 - mae: 0.3447 - val_loss: 0.2497 - val_mae: 0.3707 - lr: 1.1109e-04\n",
      "Epoch 56/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2149 - mae: 0.3473 - val_loss: 0.2494 - val_mae: 0.3710 - lr: 1.0052e-04\n",
      "Epoch 57/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2205 - mae: 0.3552 - val_loss: 0.2484 - val_mae: 0.3691 - lr: 9.0953e-05\n",
      "Epoch 58/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2178 - mae: 0.3523 - val_loss: 0.2493 - val_mae: 0.3709 - lr: 8.2297e-05\n",
      "Epoch 59/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2146 - mae: 0.3469 - val_loss: 0.2498 - val_mae: 0.3706 - lr: 7.4466e-05\n",
      "Epoch 60/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2184 - mae: 0.3495 - val_loss: 0.2485 - val_mae: 0.3701 - lr: 6.7379e-05\n",
      "Epoch 61/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2173 - mae: 0.3503 - val_loss: 0.2489 - val_mae: 0.3707 - lr: 6.0967e-05\n",
      "Epoch 62/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2123 - mae: 0.3468 - val_loss: 0.2501 - val_mae: 0.3712 - lr: 5.5166e-05\n",
      "Epoch 63/100\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.2145 - mae: 0.3469 - val_loss: 0.2492 - val_mae: 0.3703 - lr: 4.9916e-05\n",
      "Epoch 64/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2142 - mae: 0.3467 - val_loss: 0.2492 - val_mae: 0.3710 - lr: 4.5166e-05\n",
      "Epoch 65/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2134 - mae: 0.3465 - val_loss: 0.2492 - val_mae: 0.3701 - lr: 4.0868e-05\n",
      "Epoch 66/100\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2121 - mae: 0.3452 - val_loss: 0.2491 - val_mae: 0.3698 - lr: 3.6979e-05\n",
      "Epoch 67/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2142 - mae: 0.3471 - val_loss: 0.2501 - val_mae: 0.3717 - lr: 3.3460e-05\n",
      "Epoch 68/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2118 - mae: 0.3467 - val_loss: 0.2488 - val_mae: 0.3705 - lr: 3.0275e-05\n",
      "Epoch 69/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2147 - mae: 0.3491 - val_loss: 0.2492 - val_mae: 0.3706 - lr: 2.7394e-05\n",
      "Epoch 70/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2116 - mae: 0.3435 - val_loss: 0.2492 - val_mae: 0.3705 - lr: 2.4787e-05\n",
      "Epoch 71/100\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2170 - mae: 0.3515 - val_loss: 0.2500 - val_mae: 0.3716 - lr: 2.2429e-05\n",
      "Epoch 72/100\n",
      "324/326 [============================>.] - ETA: 0s - loss: 0.2142 - mae: 0.3483Restoring model weights from the end of the best epoch: 52.\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2143 - mae: 0.3484 - val_loss: 0.2501 - val_mae: 0.3714 - lr: 2.0294e-05\n",
      "Epoch 72: early stopping\n",
      "120/120 [==============================] - 1s 6ms/step\n",
      "595.425 seconds\n",
      "r2  score:  0.7569988631361022\n",
      "mse score:  0.24762691149131089\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "m_func = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "epochs = 100\n",
    "\n",
    "train_model(m_func,lr_scheduler, \n",
    "          x_train = [xo_train, xd_train], x_test = [xo_test, xd_test], \n",
    "          y_train = y_train, y_test = y_test, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "rand_seed_list = [913,425]\n",
    "model = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "\n",
    "train_model_cv(model,lr_scheduler,\n",
    "             train_pairs,x_all,x_drug,y_series,\n",
    "             epochs,rand_seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
