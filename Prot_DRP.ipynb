{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# import python libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scripts.splitting import split\n",
    "from scripts.data_wrangling import import_prot\n",
    "from scripts.feature_selection import fs_landmark\n",
    "from scripts.data_preparation import data_prep\n",
    "from scripts.data_selection import data_indexing\n",
    "from scripts.mean_model import MeanModel\n",
    "from scripts.tf_DRP_model import train_model, train_model_cv, train_model_XGBr, prediction_metrics\n",
    "\n",
    "tf.keras.utils.set_random_seed(42) # set random seeds for Python, NumPy, and TensorFlow\n",
    "\n",
    "print('Imports: Done')\n",
    "\n",
    "\n",
    "# Import and wrangle proteomics and GDSC1 drug data\n",
    "prot_df,drug_df,drug_matrix,_all_cls,_all_drugs,common_ind = import_prot()\n",
    "print('Data Wrangling: Done')\n",
    "\n",
    "## Feature Selection \n",
    "\n",
    "# create list of landmark gene proteins\n",
    "landmark_proteins = fs_landmark(prot_df)\n",
    "\n",
    "# reduce remaining features down by feature selection\n",
    "prot_df = prot_df.filter(landmark_proteins,axis=1)\n",
    "\n",
    "## Data Preparation & Selection\n",
    "x_drug, x_all, y_series = data_prep(drug_df,prot_df,common_ind)\n",
    "drug_cl_pairs =  y_series.index\n",
    "\n",
    "# Cancer blind train-test split\n",
    "rand_seed = 42 # set random seed for train-test split\n",
    "train_size = 0.75 # set train-test fraction\n",
    "\n",
    "train_pairs, test_pairs = split(rand_seed, _all_cls, _all_drugs, drug_cl_pairs, \n",
    "                                train_size, split_type='cblind')\n",
    "\n",
    "# index for each set of train-test pairs\n",
    "xo_train, xd_train, y_train, xo_test, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all,x_drug,y_series) # select train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and set parameters\n",
    "regressor = XGBRegressor(tree_method=\"hist\", seed = 42, \n",
    "                        max_depth = 350, \n",
    "                        n_estimators = 630, \n",
    "                        eta = 0.04,  \n",
    "                        min_child_weight = 3, \n",
    "                        colsample_bytree = 0.2, \n",
    "                        reg_alpha = 0.1) \n",
    "\n",
    "# run model and evaluate performance\n",
    "train_model_XGBr(regressor, x_train = [xo_train, xd_train], \n",
    "               x_test = [xo_test, xd_test], \n",
    "               y_train = y_train, y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Building\n",
    "\n",
    "# Learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "## CNN using functional API\n",
    "def build_model(learning_rate=1e-2, momentum=0.9, seed=42):\n",
    "\n",
    "    # set weight initialiser\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    \n",
    "    # omics data input\n",
    "    x_input = layers.Input(shape=(xo_train.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1,seed=seed)(x)\n",
    "    # 2nd convolution layer\n",
    "    x = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1,seed=seed)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5,seed=seed)(x)\n",
    "    x = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5,seed=seed)(x)\n",
    "    \n",
    "    # one-hot encoded drug data + dense layer\n",
    "    y_input = layers.Input(shape=(xd_train.shape[1]))\n",
    "    y = Dense(64, kernel_initializer=initializer, activation=\"relu\")(y_input) \n",
    "    \n",
    "    # Concatenate omics and encoded drug data\n",
    "    z = layers.concatenate([x, y])\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(1, kernel_initializer=initializer)(z)\n",
    "\n",
    "    model = tf.keras.Model([x_input, y_input], z)\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print('Model Building: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model without cross-validation\n",
    "\n",
    "m_func = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "epochs = 100\n",
    "\n",
    "train_model(m_func,lr_scheduler, \n",
    "          x_train = [xo_train, xd_train], x_test = [xo_test, xd_test], \n",
    "          y_train = y_train, y_test = y_test, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with cross-validation\n",
    "\n",
    "epochs = 100\n",
    "rand_seed_list = [913,425]\n",
    "model = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "\n",
    "train_model_cv(model,lr_scheduler,\n",
    "             train_pairs,x_all,x_drug,y_series,\n",
    "             epochs,rand_seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
