{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# import python libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scripts.splitting import split\n",
    "from scripts.data_wrangling import import_phos, import_SMILES\n",
    "from scripts.feature_selection import fs_functional_score, fs_atlas_landmark, true_phosphosite_filter\n",
    "from scripts.data_preparation import data_prep_SMILES, encode_SMILES\n",
    "from scripts.data_selection import data_indexing\n",
    "from scripts.mean_model import MeanModel\n",
    "from scripts.ML_DRP_model import train_model_SMILES, train_model_SMILES_cv, prep_xd, prediction_metrics\n",
    "tf.keras.utils.set_random_seed(42) # set random seeds for Python, NumPy, and TensorFlow\n",
    "\n",
    "print('Imports: Done')\n",
    "\n",
    "\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "# Import and wrangle phosphoproteomics and GDSC1 drug data\n",
    "phos_df, drug_df, drug_matrix, _all_cls, _all_drugs, common_ind  = import_phos()\n",
    "SMILES_df = import_SMILES()\n",
    "## Feature Selection \n",
    "\n",
    "# create list of phosphosites filtered by feature selection method\n",
    "#phosphosites = fs_functional_score(phos_df,95)\n",
    "phosphosites = fs_atlas_landmark(phos_df,80)\n",
    "\n",
    "# remove false positive phosphosites\n",
    "filtered_phosphosites = true_phosphosite_filter(phosphosites)\n",
    "\n",
    "# reduce remaining features down by feature selection\n",
    "phos_df = phos_df.filter(filtered_phosphosites,axis=1)\n",
    "\n",
    "## Data Preparation & Selection \n",
    "one_hot_SMILES = encode_SMILES(SMILES_df) # one-hot encode SMILES strings\n",
    "x_drug, x_all, y_series = data_prep_SMILES(drug_df,one_hot_SMILES,phos_df,common_ind)\n",
    "drug_cl_pairs =  y_series.index\n",
    "\n",
    "# convert one-hot encoded SMILES into an array of values, used for model input shape\n",
    "xd_vals = prep_xd(x_drug)\n",
    "\n",
    "# Cancer blind train-test split\n",
    "rand_seed = 42 # set random seed for train-test split\n",
    "train_size = 0.75 # set train-test fraction\n",
    "\n",
    "train_pairs, test_pairs = split(rand_seed, _all_cls, _all_drugs, drug_cl_pairs, \n",
    "                                train_size, split_type='cblind')\n",
    "\n",
    "# index for each set of train-test pairs\n",
    "xo_train, xd_train, y_train, xo_test, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all,x_drug,y_series) # select train and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Model Building\n",
    "\n",
    "# Learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "## CNN using functional API\n",
    "def build_model(learning_rate=1e-3, momentum=0, seed=42):\n",
    "\n",
    "    # set weight initialiser\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed)\n",
    "    \n",
    "    # omics data input\n",
    "    x_input = layers.Input(shape=(xo_train.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x = layers.Conv1D(filters=8, kernel_size=8, kernel_initializer=initializer, activation='relu')(x_input) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    # 2nd convolution layer\n",
    "    x = layers.Conv1D(filters=16, kernel_size=8, kernel_initializer=initializer, activation='relu')(x) \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Flatten()(x) \n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    # one-hot encoded drug data input\n",
    "    y_input = layers.Input(shape=(xd_vals.shape[1:]))\n",
    "\n",
    "    # 1st convolution layer\n",
    "    y = layers.Conv1D(filters=256, kernel_size=11, kernel_initializer=initializer, activation='relu')(y_input) \n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.MaxPooling1D(pool_size=3, strides=3)(y)\n",
    "    y = layers.Dropout(0.1)(y)\n",
    "    y = layers.Flatten()(y) \n",
    "    \n",
    "    # FC layer for xd_train\n",
    "    y = layers.Dense(64, kernel_initializer=initializer, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dense(64, kernel_initializer=initializer, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    \n",
    "    \n",
    "    # Concatenate omics and encoded drug data\n",
    "    z = layers.concatenate([x, y])\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(64, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(1, kernel_initializer=initializer)(z)\n",
    "\n",
    "    model = tf.keras.Model([x_input, y_input], z)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate, \n",
    "                                                        momentum=momentum), \n",
    "                                                        loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print('Model Building: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model without cross-validation\n",
    "\n",
    "m_func = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "epochs = 100\n",
    "\n",
    "train_model_SMILES(m_func,lr_scheduler, \n",
    "          x_train = [xo_train, xd_train], x_test = [xo_test, xd_test], \n",
    "          y_train = y_train, y_test = y_test, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with cross-validation\n",
    "\n",
    "epochs = 100\n",
    "rand_seed_list = [913,425]\n",
    "model = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "\n",
    "train_model_SMILES_cv(model,lr_scheduler,\n",
    "                      train_pairs,x_all,x_drug,y_series,\n",
    "                      epochs,rand_seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
