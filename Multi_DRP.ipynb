{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports: Done\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "# import python libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from scripts.splitting import split\n",
    "from scripts.data_wrangling import import_all\n",
    "from scripts.feature_selection import fs_functional_score, fs_atlas_landmark, fs_landmark, true_phosphosite_filter\n",
    "from scripts.data_preparation import data_prep\n",
    "from scripts.data_selection import data_indexing\n",
    "from scripts.mean_model import MeanModel\n",
    "from scripts.tf_DRP_model import train_model_multi, train_model_multi_cv\n",
    "\n",
    "tf.keras.utils.set_random_seed(42) # set random seeds for Python, NumPy, and TensorFlow\n",
    "\n",
    "print('Imports: Done')\n",
    "\n",
    "# Import and wrangle phosphoproteomics, proteomics and GDSC1 drug data\n",
    "phos_df, prot_df, drug_df, drug_matrix, _all_cls, _all_drugs, common_ind = import_all()\n",
    "\n",
    "## Feature Selection \n",
    "\n",
    "# create list of landmark phosphosites\n",
    "phosphosites = fs_functional_score(phos_df,95)\n",
    "#phosphosites = fs_atlas_landmark(phos_df,80)\n",
    "# remove false positive phosphosites\n",
    "filtered_phosphosites = true_phosphosite_filter(phosphosites)\n",
    "# reduce remaining features down by feature selection\n",
    "phos_df = phos_df.filter(filtered_phosphosites,axis=1)\n",
    "\n",
    "# create list of landmark gene proteins\n",
    "landmark_proteins = fs_landmark(prot_df)\n",
    "# reduce remaining features down by feature selection\n",
    "prot_df = prot_df.filter(landmark_proteins,axis=1)\n",
    "\n",
    "## Data Preparation & Selection\n",
    "x_drug, x_all_phos, y_series = data_prep(drug_df,phos_df,common_ind)\n",
    "x_drug, x_all_prot, y_series = data_prep(drug_df,prot_df,common_ind)\n",
    "drug_cl_pairs =  y_series.index\n",
    "\n",
    "# Cancer blind train-test split\n",
    "rand_seed = 42 # set random seed for train-test split\n",
    "train_size = 0.75 # set train-test fraction\n",
    "\n",
    "train_pairs, test_pairs = split(rand_seed, _all_cls, _all_drugs, drug_cl_pairs, \n",
    "                                train_size, split_type='cblind')\n",
    "\n",
    "# index for each set of train-test pairs\n",
    "xo_train_phos, xd_train, y_train, xo_test_phos, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all_phos,x_drug,y_series) \n",
    "xo_train_prot, xd_train, y_train, xo_test_prot, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all_prot,x_drug,y_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Building: Done\n"
     ]
    }
   ],
   "source": [
    "## Model Building\n",
    "\n",
    "# Learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "## CNN using functional API\n",
    "def build_model(learning_rate=1e-2, momentum=0.9, seed=42):\n",
    "\n",
    "    # set weight initialiser\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    \n",
    "    ## Drug input\n",
    "    # one-hot encoded drug data + dense layer\n",
    "    y_input = layers.Input(shape=(xd_train.shape[1]))\n",
    "    y = Dense(256, kernel_initializer=initializer, activation=\"relu\")(y_input) \n",
    "    \n",
    "    \n",
    "    ## Phosphoproteomics branch\n",
    "    # phosphoproteomics data input\n",
    "    x_input_phospho = layers.Input(shape=(xo_train_phos.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x_1 = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input_phospho) \n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.MaxPooling1D()(x_1)\n",
    "    x_1 = layers.Dropout(0.1)(x_1)\n",
    "    # 2nd convolution layer\n",
    "    x_1 = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.MaxPooling1D()(x_1)\n",
    "    x_1 = layers.Dropout(0.1)(x_1)\n",
    "    x_1 = layers.Flatten()(x_1)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x_1 = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.Dropout(0.5)(x_1)\n",
    "    x_1 = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.Dropout(0.5)(x_1)\n",
    "    \n",
    "    \n",
    "    ## Proteomics branch\n",
    "    # proteomics data input\n",
    "    x_input_prot = layers.Input(shape=(xo_train_prot.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x_2 = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input_prot) \n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.MaxPooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(0.1)(x_2)\n",
    "    # 2nd convolution layer\n",
    "    x_2 = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.MaxPooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(0.1)(x_2)\n",
    "    x_2 = layers.Flatten()(x_2)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x_2 = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.Dropout(0.5)(x_2)\n",
    "    x_2 = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.Dropout(0.5)(x_2) \n",
    "    \n",
    "    # Concat\n",
    "    z = layers.concatenate([x_1, x_2, y])\n",
    "    z = layers.Dense(128, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(128, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(1, kernel_initializer=initializer)(z)\n",
    "\n",
    "    model = tf.keras.Model([x_input_phospho, x_input_prot, y_input], z)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate, \n",
    "                                                        momentum=momentum), \n",
    "                                                        loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print('Model Building: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "324/324 [==============================] - 22s 39ms/step - loss: 0.9690 - mae: 0.6768 - val_loss: 0.5860 - val_mae: 0.6126 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "324/324 [==============================] - 12s 37ms/step - loss: 0.4618 - mae: 0.5230 - val_loss: 0.4278 - val_mae: 0.5252 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.4179 - mae: 0.4965 - val_loss: 1.4666 - val_mae: 0.9827 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.4087 - mae: 0.4920 - val_loss: 0.3612 - val_mae: 0.4641 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.3890 - mae: 0.4806 - val_loss: 0.8593 - val_mae: 0.8112 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.4332 - mae: 0.5107 - val_loss: 14412032.0000 - val_mae: 1116.9795 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.3916 - mae: 0.4839 - val_loss: 0.3294 - val_mae: 0.4485 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.4007 - mae: 0.4882 - val_loss: 0.2896 - val_mae: 0.4109 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.4489 - mae: 0.5185 - val_loss: 0.5827 - val_mae: 0.6439 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.3607 - mae: 0.4607 - val_loss: 301326272.0000 - val_mae: 5446.9775 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.3722 - mae: 0.4664 - val_loss: 8.6692 - val_mae: 0.9175 - lr: 0.0090\n",
      "Epoch 12/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.3805 - mae: 0.4724 - val_loss: 0.2622 - val_mae: 0.3927 - lr: 0.0082\n",
      "Epoch 13/100\n",
      "324/324 [==============================] - 12s 37ms/step - loss: 0.3723 - mae: 0.4675 - val_loss: 0.2556 - val_mae: 0.3889 - lr: 0.0074\n",
      "Epoch 14/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.3450 - mae: 0.4517 - val_loss: 0.3664 - val_mae: 0.4817 - lr: 0.0067\n",
      "Epoch 15/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.3521 - mae: 0.4587 - val_loss: 0.3088 - val_mae: 0.4392 - lr: 0.0061\n",
      "Epoch 16/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.3299 - mae: 0.4421 - val_loss: 0.2194 - val_mae: 0.3532 - lr: 0.0055\n",
      "Epoch 17/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.3273 - mae: 0.4433 - val_loss: 0.2187 - val_mae: 0.3501 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2989 - mae: 0.4157 - val_loss: 0.2683 - val_mae: 0.3968 - lr: 0.0045\n",
      "Epoch 19/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2808 - mae: 0.4037 - val_loss: 0.5330 - val_mae: 0.6039 - lr: 0.0041\n",
      "Epoch 20/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2865 - mae: 0.4099 - val_loss: 0.2445 - val_mae: 0.3664 - lr: 0.0037\n",
      "Epoch 21/100\n",
      "324/324 [==============================] - 12s 36ms/step - loss: 0.2730 - mae: 0.3966 - val_loss: 0.3348 - val_mae: 0.4573 - lr: 0.0033\n",
      "Epoch 22/100\n",
      "324/324 [==============================] - 12s 36ms/step - loss: 0.2543 - mae: 0.3807 - val_loss: 0.2316 - val_mae: 0.3576 - lr: 0.0030\n",
      "Epoch 23/100\n",
      "324/324 [==============================] - 12s 37ms/step - loss: 0.2604 - mae: 0.3907 - val_loss: 0.2796 - val_mae: 0.4120 - lr: 0.0027\n",
      "Epoch 24/100\n",
      "324/324 [==============================] - 12s 37ms/step - loss: 0.2566 - mae: 0.3835 - val_loss: 0.2700 - val_mae: 0.3949 - lr: 0.0025\n",
      "Epoch 25/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2572 - mae: 0.3842 - val_loss: 0.2207 - val_mae: 0.3514 - lr: 0.0022\n",
      "Epoch 26/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2520 - mae: 0.3791 - val_loss: 0.2954 - val_mae: 0.4219 - lr: 0.0020\n",
      "Epoch 27/100\n",
      "324/324 [==============================] - 10s 32ms/step - loss: 0.2507 - mae: 0.3768 - val_loss: 0.2461 - val_mae: 0.3738 - lr: 0.0018\n",
      "Epoch 28/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2551 - mae: 0.3814 - val_loss: 0.2355 - val_mae: 0.3559 - lr: 0.0017\n",
      "Epoch 29/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2504 - mae: 0.3790 - val_loss: 0.2334 - val_mae: 0.3654 - lr: 0.0015\n",
      "Epoch 30/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2460 - mae: 0.3740 - val_loss: 0.2181 - val_mae: 0.3460 - lr: 0.0014\n",
      "Epoch 31/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2403 - mae: 0.3694 - val_loss: 0.2664 - val_mae: 0.4023 - lr: 0.0012\n",
      "Epoch 32/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2421 - mae: 0.3695 - val_loss: 0.2173 - val_mae: 0.3446 - lr: 0.0011\n",
      "Epoch 33/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2375 - mae: 0.3672 - val_loss: 0.2368 - val_mae: 0.3582 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2407 - mae: 0.3684 - val_loss: 0.2269 - val_mae: 0.3498 - lr: 9.0718e-04\n",
      "Epoch 35/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2401 - mae: 0.3688 - val_loss: 0.2281 - val_mae: 0.3580 - lr: 8.2085e-04\n",
      "Epoch 36/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2355 - mae: 0.3642 - val_loss: 0.2200 - val_mae: 0.3457 - lr: 7.4273e-04\n",
      "Epoch 37/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2254 - mae: 0.3575 - val_loss: 0.2271 - val_mae: 0.3504 - lr: 6.7205e-04\n",
      "Epoch 38/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2305 - mae: 0.3596 - val_loss: 0.2218 - val_mae: 0.3469 - lr: 6.0810e-04\n",
      "Epoch 39/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2321 - mae: 0.3612 - val_loss: 0.2320 - val_mae: 0.3525 - lr: 5.5023e-04\n",
      "Epoch 40/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2285 - mae: 0.3596 - val_loss: 0.2211 - val_mae: 0.3496 - lr: 4.9787e-04\n",
      "Epoch 41/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2258 - mae: 0.3553 - val_loss: 0.2157 - val_mae: 0.3432 - lr: 4.5049e-04\n",
      "Epoch 42/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2278 - mae: 0.3558 - val_loss: 0.2256 - val_mae: 0.3505 - lr: 4.0762e-04\n",
      "Epoch 43/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2270 - mae: 0.3574 - val_loss: 0.2240 - val_mae: 0.3479 - lr: 3.6883e-04\n",
      "Epoch 44/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2234 - mae: 0.3559 - val_loss: 0.2193 - val_mae: 0.3460 - lr: 3.3373e-04\n",
      "Epoch 45/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2286 - mae: 0.3578 - val_loss: 0.2186 - val_mae: 0.3454 - lr: 3.0197e-04\n",
      "Epoch 46/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2313 - mae: 0.3611 - val_loss: 0.2202 - val_mae: 0.3467 - lr: 2.7324e-04\n",
      "Epoch 47/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2296 - mae: 0.3580 - val_loss: 0.2183 - val_mae: 0.3455 - lr: 2.4723e-04\n",
      "Epoch 48/100\n",
      "324/324 [==============================] - 11s 33ms/step - loss: 0.2210 - mae: 0.3513 - val_loss: 0.2174 - val_mae: 0.3440 - lr: 2.2371e-04\n",
      "Epoch 49/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2306 - mae: 0.3606 - val_loss: 0.2172 - val_mae: 0.3445 - lr: 2.0242e-04\n",
      "Epoch 50/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2265 - mae: 0.3565 - val_loss: 0.2184 - val_mae: 0.3450 - lr: 1.8316e-04\n",
      "Epoch 51/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2271 - mae: 0.3570 - val_loss: 0.2184 - val_mae: 0.3449 - lr: 1.6573e-04\n",
      "Epoch 52/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2249 - mae: 0.3569 - val_loss: 0.2194 - val_mae: 0.3458 - lr: 1.4996e-04\n",
      "Epoch 53/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2251 - mae: 0.3554 - val_loss: 0.2209 - val_mae: 0.3466 - lr: 1.3569e-04\n",
      "Epoch 54/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2198 - mae: 0.3511 - val_loss: 0.2183 - val_mae: 0.3452 - lr: 1.2277e-04\n",
      "Epoch 55/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2204 - mae: 0.3518 - val_loss: 0.2182 - val_mae: 0.3449 - lr: 1.1109e-04\n",
      "Epoch 56/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2283 - mae: 0.3589 - val_loss: 0.2193 - val_mae: 0.3460 - lr: 1.0052e-04\n",
      "Epoch 57/100\n",
      "324/324 [==============================] - 11s 34ms/step - loss: 0.2260 - mae: 0.3559 - val_loss: 0.2186 - val_mae: 0.3453 - lr: 9.0953e-05\n",
      "Epoch 58/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2212 - mae: 0.3534 - val_loss: 0.2192 - val_mae: 0.3454 - lr: 8.2297e-05\n",
      "Epoch 59/100\n",
      "324/324 [==============================] - 11s 35ms/step - loss: 0.2253 - mae: 0.3569 - val_loss: 0.2176 - val_mae: 0.3446 - lr: 7.4466e-05\n",
      "Epoch 60/100\n",
      "324/324 [==============================] - 12s 37ms/step - loss: 0.2287 - mae: 0.3579 - val_loss: 0.2176 - val_mae: 0.3445 - lr: 6.7379e-05\n",
      "Epoch 61/100\n",
      "324/324 [==============================] - ETA: 0s - loss: 0.2217 - mae: 0.3542Restoring model weights from the end of the best epoch: 41.\n",
      "324/324 [==============================] - 12s 36ms/step - loss: 0.2217 - mae: 0.3542 - val_loss: 0.2205 - val_mae: 0.3461 - lr: 6.0967e-05\n",
      "Epoch 61: early stopping\n",
      "121/121 [==============================] - 1s 8ms/step\n",
      "693.541 seconds\n",
      "r2  score:  0.7749666300582729\n",
      "mse score:  0.21565053272681867\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "m_func = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "epochs = 100\n",
    "\n",
    "train_model_multi(m_func,lr_scheduler, \n",
    "          x_train = [xo_train_phos, xo_train_prot, xd_train], \n",
    "          x_test = [xo_test_phos, xo_test_prot, xd_test], \n",
    "          y_train = y_train, y_test = y_test, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 913:\n",
      "\n",
      "K-fold 1\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 22: early stopping\n",
      "239.232 seconds\n",
      "r2  score:  0.7598200840225259\n",
      "mse score:  0.22530355653307996\n",
      "-----\n",
      "K-fold 2\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 55: early stopping\n",
      "589.140 seconds\n",
      "r2  score:  0.8293708851389254\n",
      "mse score:  0.17516349957633576\n",
      "-----\n",
      "K-fold 3\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 20: early stopping\n",
      "215.783 seconds\n",
      "r2  score:  0.819197725492145\n",
      "mse score:  0.1917555424084669\n",
      "-----\n",
      "r2 Scores for seed 913:\n",
      "0.7598200840225259\n",
      "0.8293708851389254\n",
      "0.819197725492145\n",
      "\n",
      "mean r2: 0.8028\n",
      "-----\n",
      "\n",
      "Seed 425:\n",
      "\n",
      "K-fold 1\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 29: early stopping\n",
      "309.722 seconds\n",
      "r2  score:  0.8297271131627653\n",
      "mse score:  0.17224190020367272\n",
      "-----\n",
      "K-fold 2\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 13: early stopping\n",
      "137.880 seconds\n",
      "r2  score:  0.8205783556686086\n",
      "mse score:  0.18656946527188786\n",
      "-----\n",
      "K-fold 3\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 16: early stopping\n",
      "171.110 seconds\n",
      "r2  score:  0.8276777285599122\n",
      "mse score:  0.17075790491581594\n",
      "-----\n",
      "r2 Scores for seed 425:\n",
      "0.8297271131627653\n",
      "0.8205783556686086\n",
      "0.8276777285599122\n",
      "\n",
      "mean r2: 0.826\n",
      "-----\n",
      "\n",
      "Final results:\n",
      "913: 0.8028\n",
      "425: 0.826\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "rand_seed_list = [913,425]\n",
    "model = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "\n",
    "train_model_multi_cv(model,lr_scheduler,train_pairs,\n",
    "             x_all = [x_all_phos,x_all_prot],\n",
    "             x_drug = x_drug,y_series = y_series,\n",
    "             epochs=epochs,rand_seed_list=rand_seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
