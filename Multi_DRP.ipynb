{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# import python libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from importlib import reload\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from scripts.splitting import split\n",
    "from scripts.data_wrangling import import_all\n",
    "from scripts.feature_selection import fs_functional_score, fs_atlas_landmark, fs_landmark, true_phosphosite_filter\n",
    "from scripts.data_preparation import data_prep\n",
    "from scripts.data_selection import data_indexing\n",
    "from scripts.mean_model import MeanModel\n",
    "from scripts.tf_DRP_model import train_model_multi, train_model_multi_cv, prediction_metrics\n",
    "\n",
    "tf.keras.utils.set_random_seed(42) # set random seeds for Python, NumPy, and TensorFlow\n",
    "\n",
    "print('Imports: Done')\n",
    "\n",
    "# Import and wrangle phosphoproteomics, proteomics and GDSC1 drug data\n",
    "phos_df, prot_df, drug_df, drug_matrix, _all_cls, _all_drugs, common_ind = import_all()\n",
    "\n",
    "## Feature Selection \n",
    "\n",
    "# create list of phosphosites filtered by feature selection method\n",
    "phosphosites = fs_functional_score(phos_df,95)\n",
    "#phosphosites = fs_atlas_landmark(phos_df,80)\n",
    "# remove false positive phosphosites\n",
    "filtered_phosphosites = true_phosphosite_filter(phosphosites)\n",
    "# reduce remaining features down by feature selection\n",
    "phos_df = phos_df.filter(filtered_phosphosites,axis=1)\n",
    "\n",
    "# create list of landmark gene proteins\n",
    "landmark_proteins = fs_landmark(prot_df)\n",
    "# reduce remaining features down by feature selection\n",
    "prot_df = prot_df.filter(landmark_proteins,axis=1)\n",
    "\n",
    "## Data Preparation & Selection\n",
    "x_drug, x_all_phos, y_series = data_prep(drug_df,phos_df,common_ind)\n",
    "x_drug, x_all_prot, y_series = data_prep(drug_df,prot_df,common_ind)\n",
    "drug_cl_pairs =  y_series.index\n",
    "\n",
    "# Cancer blind train-test split\n",
    "rand_seed = 42 # set random seed for train-test split\n",
    "train_size = 0.75 # set train-test fraction\n",
    "\n",
    "train_pairs, test_pairs = split(rand_seed, _all_cls, _all_drugs, drug_cl_pairs, \n",
    "                                train_size, split_type='cblind')\n",
    "\n",
    "# index for each set of train-test pairs\n",
    "xo_train_phos, xd_train, y_train, xo_test_phos, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all_phos,x_drug,y_series) \n",
    "xo_train_prot, xd_train, y_train, xo_test_prot, xd_test, y_test = data_indexing(train_pairs,test_pairs,x_all_prot,x_drug,y_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Model Building\n",
    "\n",
    "# Learning rate schedule function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "## CNN using functional API\n",
    "def build_model(learning_rate=1e-2, momentum=0.9, seed=42):\n",
    "\n",
    "    # set weight initialiser\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    \n",
    "    ## Drug input\n",
    "    # one-hot encoded drug data + dense layer\n",
    "    y_input = layers.Input(shape=(xd_train.shape[1]))\n",
    "    y = Dense(256, kernel_initializer=initializer, activation=\"relu\")(y_input) \n",
    "    \n",
    "    \n",
    "    ## Phosphoproteomics branch\n",
    "    # phosphoproteomics data input\n",
    "    x_input_phospho = layers.Input(shape=(xo_train_phos.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x_1 = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input_phospho) \n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.MaxPooling1D()(x_1)\n",
    "    x_1 = layers.Dropout(0.1)(x_1)\n",
    "    # 2nd convolution layer\n",
    "    x_1 = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.MaxPooling1D()(x_1)\n",
    "    x_1 = layers.Dropout(0.1)(x_1)\n",
    "    x_1 = layers.Flatten()(x_1)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x_1 = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.Dropout(0.5)(x_1)\n",
    "    x_1 = layers.Dense(256, kernel_initializer=initializer, activation='relu')(x_1)\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_1 = layers.Dropout(0.5)(x_1)\n",
    "    \n",
    "    \n",
    "    ## Proteomics branch\n",
    "    # proteomics data input\n",
    "    x_input_prot = layers.Input(shape=(xo_train_prot.shape[1],1))\n",
    "    # 1st convolution layer\n",
    "    x_2 = layers.Conv1D(filters=8, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_input_prot) \n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.MaxPooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(0.1)(x_2)\n",
    "    # 2nd convolution layer\n",
    "    x_2 = layers.Conv1D(filters=16, kernel_size=4, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.MaxPooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(0.1)(x_2)\n",
    "    x_2 = layers.Flatten()(x_2)\n",
    "\n",
    "    # FC layer for xo_train\n",
    "    x_2 = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.Dropout(0.5)(x_2)\n",
    "    x_2 = layers.Dense(128, kernel_initializer=initializer, activation='relu')(x_2)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_2 = layers.Dropout(0.5)(x_2) \n",
    "    \n",
    "    ## Concatenate omics and encoded drug data\n",
    "    z = layers.concatenate([x_1, x_2, y])\n",
    "    z = layers.Dense(128, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(128, kernel_initializer=initializer, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dense(1, kernel_initializer=initializer)(z)\n",
    "\n",
    "    model = tf.keras.Model([x_input_phospho, x_input_prot, y_input], z)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate, \n",
    "                                                        momentum=momentum), \n",
    "                                                        loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print('Model Building: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model without cross-validation\n",
    "\n",
    "m_func = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "epochs = 100\n",
    "\n",
    "train_model_multi(m_func,lr_scheduler, \n",
    "          x_train = [xo_train_phos, xo_train_prot, xd_train], \n",
    "          x_test = [xo_test_phos, xo_test_prot, xd_test], \n",
    "          y_train = y_train, y_test = y_test, \n",
    "          epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with cross-validation\n",
    "\n",
    "epochs = 100\n",
    "rand_seed_list = [913,425]\n",
    "model = build_model() # set model and parameters\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) # set learning rate scheduler\n",
    "\n",
    "train_model_multi_cv(model,lr_scheduler,train_pairs,\n",
    "             x_all = [x_all_phos,x_all_prot],\n",
    "             x_drug = x_drug,y_series = y_series,\n",
    "             epochs=epochs,rand_seed_list=rand_seed_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
